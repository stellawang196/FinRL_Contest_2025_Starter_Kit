**New notes for clarifications**

The basic requirement is that your model should be able to interact with the environment. The code for training agent and ensemble is just an example solution for your reference.

1. You are free to apply any method for ensemble learning. (For example, You can add new agents, use different ensemble algorithms, adjust hyperparameters, etc) The code provided is just to help get started and we encourage innovation.
2. You are not required to stick to the 8 features we provide. But for evaluation purpose, please make sure that your new technical factors, if any, can be calculated based on the unseen data. Please include this code and state clearly in readme.
3. We will use the provided environment to evaluate. So it is not encouraged to change the existing parameters in the environment. However, you can fully utilize the environment settings and the massively parallel simulation.
4. To encourage innovation, if you want to add new mechanisms or use the unused settings (e.g. short sell) in the environment, please also submit your environment, ensure it works with your agent for evaluation, and describe the new changes in the readme.


# FinRL Task 1
This task aims to develop robust and effective trading agents for cryptocurrencies using ensemble methods. Participants are expected to explore innovative ensemble methods for single cryptocurrency trading. They are also encouraged to take advantage of the power of massively parallel simulations by utilizing the provided vectorized environments.

The code in this repo contains a comprehensive starter kit to get started with training single agents in massively parallel environments.

There is an example of ensemble method to use the majority voting approach in the tutorial, [Task 1 Crypto Trading Ensemble](https://github.com/Open-Finance-Lab/FinRL_Contest_2024/tree/main/Tutorials/Task_1_tutorial). Later, we will provide submission format and example, which will use this tutorial.

## Dataset
A dataset containing second-level Limit Order Book (LOB) data for Bitcoin is provided. Please download [here](https://drive.google.com/drive/folders/1Okd8fyB7n93N1Z5HEnlpb-q8x5FfSF1Z?usp=sharing). All of the datasets required to train DRL agents are in the data directory, please download this into the Task_1 starter dir 

The dataset contains the following files: 
- BTC_1sec.csv : 1 second level LOB BTC data
- BTC_1sec_predict.npy : strong RNN factors generated on the csv dataset to train the DRL agent

We generate the strong RNN features by first training a RNN model on 60% of the dataset. The RNN is trained to predict future price movements using 101 formulaic alphas [1]. The strong factors generated by RNN are used as technical factors to train the FinRL agent. The FinRL agent will be trained on the remaining dataset, and then evaluated on a separate test set. The RNN we use combines LSTM and GRU networks for better data modeling. It uses a MLP input layer to process the alpha101 features followed by two RNN networks to model the data.

The `BTC_1sec.csv` contains all data used to train the RNN model and FinRL agent. Notice that the timestamps in this dataset has been processed and are not the true timestamps. 

## Starter Kit Descriptions

This starter kit demonstrates how to use the provided code. We provide you with RNN generated strong factors to use for the DRL agent. You are welcome to experiment with various ensemble configurations that yield optimal results. 

The starter kit includes:
- `trade_simulator.py`: Contains a market replay simulator for single commodity.
  - Class `TradeSimulator`: A training-focused market replay simulator, complying with the older gym-style API requirements.
  - Class `EvalTradeSimulator`: A market replay simulator for evaluation.

- `erl_config.py`: Configuration file for reinforcement learning training.

- `erl_replay_buffer.py`: Serves as a training dataset for the reinforcement learning market replay simulator.

- `erl_agent.py`: Contains the DQN class algorithm for reinforcement learning.

- `erl_net.py`: Neural network structures used in the reinforcement learning algorithm.

- `erl_run.py`: Loads the simulator and trains the reinforcement learning agent.

- `erl_evaluator.py`: Evaluates the performance of the reinforcement learning agent.

- `metrics.py`: Contains some metrics for evaluation.

- `task1_ensemble.py`: This file contains code that trains multiple models and then saves them to be tested during evaluation.

- `task1_eval.py`: This file contains code that loads your ensemble and simulates trading over a validation dataset. You may create this validation dataset by holding out a part of the training data.

We will provide the evaluation code soon. 

**Notes:**
The basic requirement is that your model should be able to interact with the environment. The code for training agent and ensemble is just an example solution for your reference. 
1. You are free to apply any method for ensemble learning. (For example, You can add new agents, use different ensemble algorithms, adjust hyperparameters, etc) The code provided is just to help get started and we encourage innovation.
2. You are not required to stick to the 8 features we provide. But for evaluation purpose, please make sure that your new technical factors, if any, can be calculated based on the unseen data. Please include this code and state clearly in readme.
3. We will use the provided environment to evaluate. So it is not encouraged to change the existing parameters in the environment. However, you can fully utilize the environment settings and the massively parallel simulation.
4. To encourage innovation, if you want to add new mechanisms or use the unused settings (e.g. short sell, different voting mechanisms for the ensemble) in the environment, please also submit your environment, ensure it works with your agent for evaluation, and describe the new changes in the readme.

## Evaluation Template
We provide an evaluation template that you may use to test your ensemble models. You may change `"predict_ary_path": "BTC_1sec_predict.npy"` in the env_args object to point to a validation subset of the predict ary path which will let you test your model on out of sample data. 

The evaluation kit is in `task1_eval.py` and loads your models then uses a simple voting scheme to perform ensemble actions. As mentioned above, if you change the voting scheme, please be sure to submit your environment code. You may change following:
- `_ensemble_action` you may substitute this for another ensemble action function. Please submit your action functions so that we may use them to evaluate your models.
- `save_path`: this is the path of your saved models for the ensemble saved in the task1_train file. 
- `dataset_path`: please provide a path to a validation dataset. We recommend using a subset of `BTC_1sec_predict.npy` for validation.
- `args.net_dims`: we use a default setting of (128,128,128) but you may choose to use different models. Please provide your settings so that we can evaluate them properly.

## Evaluation and Submission guidelines
The initial cash is $1 million.

Please submit all your models and the scripts to load and test the models.

Please provide a readme that describes your submission and explains important things to note when running it so that we can ensure we run your submission as it was intended.

```
├── finrl-contest-task-2 
│ ├── trained_models # Your trained component agent weights.
│ ├── task1_ensemble.py # File for implementing the ensemble method 
│ ├── task1_eval.py # a template evaluation file. Please submit your evaluation code as well.
│ ├── trade_simulator.py # File for the environment. Please submit it if you modified the provided env.
│ ├── READE.md # File to explain the your code
│ ├── requirements.txt # Have it if adding any new packages
│ ├── And any additional scripts you create
```
We will provide the template for `task1_ensemble.py` and `task1_test.py` based on the tutorial example solution soon.


## Evaluation
For evaluatiuon, we will run your ensemble agents on a test set and compare the results using metrics like cumulative return, win loss rate and sharpe ratio (subject to change). 


[1] Zura Kakushadze. 2016. 101 formulaic alphas. arXiv preprint arXiv:1601.00991
